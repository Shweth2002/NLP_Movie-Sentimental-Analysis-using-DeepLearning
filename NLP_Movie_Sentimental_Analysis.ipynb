{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec3ee0f5",
   "metadata": {},
   "source": [
    "# Project in NLP we will be building a text classification model using NLP. The dataset we will be using is the IMDB dataset which is prebuilt in keras for faster execution.\n",
    "\n",
    "The dataset contains movie data along with genres.\n",
    "\n",
    "The task we would be doing is to classify the movie in their respective genres.\n",
    "\n",
    "For the sake of simplicity, we use the first 10,000 records. You are free to explore with more data. The execution time increases with more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09098763",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: netron in c:\\users\\shwetha v\\anaconda3\\lib\\site-packages (6.5.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fed6bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import netron\n",
    "import nltk\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from nltk.corpus import stopwords\n",
    "from keras.utils import to_categorical\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.datasets import imdb\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f308675e",
   "metadata": {},
   "source": [
    "This is a dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a list of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd70f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_target), (test_data, test_target) = imdb.load_data(num_words=10000)\n",
    "dt = np.concatenate((train_data, test_data), axis=0)\n",
    "tar = np.concatenate((train_target, test_target), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "accfdec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 0 0 0]\n",
      "[1 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#print(train_data)\n",
    "print(test_target)\n",
    "print(tar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39070da8",
   "metadata": {},
   "source": [
    "The function convert(): converts the words into vectors for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f16f40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(sequences, dimension = 10000):\n",
    " results = np.zeros((len(sequences), dimension))\n",
    " print(results.shape)\n",
    " print(results)\n",
    " for i, sequence in enumerate(sequences):\n",
    "  results[i, sequence] = 1\n",
    " return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4236138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10000)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "dt = convert(dt)\n",
    "tar = np.array(tar).astype(\"float32\")\n",
    "test_x = dt[:9000]\n",
    "test_y = tar[:9000]\n",
    "train_x = dt[9000:]\n",
    "train_y = tar[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9410caa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 50)                500050    \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 505,201\n",
      "Trainable params: 505,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Serving 'C:\\Users\\Shwetha V\\Desktop\\nlp_model.h5' at http://localhost:8080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('localhost', 8080)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "# Input - Layer\n",
    "model.add(layers.Dense(50, activation = \"relu\", input_shape=(10000, )))\n",
    "# Hidden - Layers\n",
    "model.add(layers.Dropout(0.4, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation = \"relu\"))\n",
    "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation = \"relu\"))\n",
    "# Output- Layer\n",
    "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "model.summary()\n",
    "\n",
    "model.save(r'C:\\Users\\Shwetha V\\Desktop\\nlp_model.h5')\n",
    "\n",
    "netron.start(r'C:\\Users\\Shwetha V\\Desktop\\nlp_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a75c24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "82/82 [==============================] - 9s 70ms/step - loss: 0.4323 - accuracy: 0.8035 - val_loss: 0.2647 - val_accuracy: 0.8934\n",
      "Epoch 2/2\n",
      "82/82 [==============================] - 4s 51ms/step - loss: 0.2299 - accuracy: 0.9132 - val_loss: 0.2708 - val_accuracy: 0.8900\n",
      "Test-Accuracy: 0.8917222023010254\n"
     ]
    }
   ],
   "source": [
    "# compiling the model\n",
    " \n",
    "model.compile(\n",
    " optimizer = \"adam\",\n",
    " loss = \"binary_crossentropy\",\n",
    " metrics = [\"accuracy\"]\n",
    ")\n",
    "results = model.fit(\n",
    " train_x, train_y,\n",
    " epochs= 2,\n",
    " batch_size = 500,\n",
    " validation_data = (test_x, test_y)\n",
    ")\n",
    "\n",
    "print(\"Test-Accuracy:\", np.mean(results.history[\"val_accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26ef3d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 1s 3ms/step\n",
      "2689\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "preds=model.predict(test_x)\n",
    "\n",
    "best_index = np.argmax(preds, axis=0)[0]\n",
    "print(best_index)\n",
    "print(test_target[best_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d0b4548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 1s 4ms/step - loss: 0.2708 - accuracy: 0.8900\n",
      "Accuracy [0.2708359360694885, 0.8899999856948853]\n"
     ]
    }
   ],
   "source": [
    "accuracy=model.evaluate(test_x, test_y)\n",
    "print('Accuracy',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd996b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter some text: This a fantastic movie of three prisoners who become famous. One of the actors is george clooney and I'm not a fan but this roll is not bad. Another good thing about the movie is the soundtrack (The man of constant sorrow). I recommand this movie to everybody. Greetings Bart\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Positive sentiment\n"
     ]
    }
   ],
   "source": [
    "# Define a function to preprocess user input\n",
    "def preprocess_input(text):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Convert the tokens to integers using the IMDb dataset's word index\n",
    "    word_index = imdb.get_word_index()\n",
    "    int_tokens = []\n",
    "    num_words=10000\n",
    "    for token in filtered_tokens:\n",
    "        if token in word_index and word_index[token] < num_words:\n",
    "            int_tokens.append(word_index[token])\n",
    "\n",
    "    # Pad the sequence\n",
    "    padded_seq = sequence.pad_sequences([int_tokens], maxlen=num_words)\n",
    "\n",
    "    return padded_seq\n",
    "\n",
    "# Take user input and preprocess it\n",
    "user_input = input('Enter some text: ')\n",
    "preprocessed_input = preprocess_input(user_input)\n",
    "\n",
    "# Predict the sentiment of the input\n",
    "prediction = model.predict(preprocessed_input)[0][0]\n",
    "\n",
    "if prediction > 0.5:\n",
    "    print('Positive sentiment')\n",
    "else:\n",
    "    print('Negative sentiment')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "49b171cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter some text:  An awful film! It must have been up against some real stinkers to be nominated for the Golden Globe. They've taken the story of the first famous female Renaissance painter and mangled it beyond recognition. My complaint is not that they've taken liberties with the facts; if the story were good, that would perfectly fine. But it's simply bizarre -- by all accounts the true story of this artist would have made for a far better film, so why did they come up with this dishwater-dull script? I suppose there weren't enough naked people in the factual version. It's hurriedly capped off in the end with a summary of the artist's life -- we could have saved ourselves a couple of hours if they'd favored the rest of the film with same brevity.\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Negative sentiment\n"
     ]
    }
   ],
   "source": [
    "# Define a function to preprocess user input\n",
    "def preprocess_input(text):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Convert the tokens to integers using the IMDb dataset's word index\n",
    "    word_index = imdb.get_word_index()\n",
    "    int_tokens = []\n",
    "    num_words=10000\n",
    "    for token in filtered_tokens:\n",
    "        if token in word_index and word_index[token] < num_words:\n",
    "            int_tokens.append(word_index[token])\n",
    "\n",
    "    # Pad the sequence\n",
    "    padded_seq = sequence.pad_sequences([int_tokens], maxlen=num_words)\n",
    "\n",
    "    return padded_seq\n",
    "\n",
    "# Take user input and preprocess it\n",
    "user_input = input('Enter some text: ')\n",
    "preprocessed_input = preprocess_input(user_input)\n",
    "\n",
    "# Predict the sentiment of the input\n",
    "prediction = model.predict(preprocessed_input)[0][0]\n",
    "\n",
    "if prediction > 0.5:\n",
    "    print('Positive sentiment')\n",
    "else:\n",
    "    print('Negative sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7826bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4ee914",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
